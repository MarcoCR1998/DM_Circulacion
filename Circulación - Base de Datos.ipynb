{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOe8d7aa1pbMrSlZZTX3wT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Pipeline : Manejo de Datos - Circulación**"],"metadata":{"id":"YyiVljDZeck3"}},{"cell_type":"markdown","source":["\n","Este proyecto detalla el proceso completo llevado a cabo por el equipo consultor PAIT#11 para desarrollar un pipeline que permite recrear cada uno de los pasos realizados en la construcción de una base de datos destinada al Departamento de Circulación. El objetivo principal es garantizar la trazabilidad, eficiencia y automatización en la recopilación, transformación y almacenamiento de datos, facilitando así la toma de decisiones basada en información precisa y actualizada.  \n","\n","A lo largo de este pipeline, se abordan etapas clave como la extracción de datos de diversas fuentes, la limpieza y transformación de los mismos para asegurar su consistencia, y finalmente su carga en la estructura diseñada. Además, se consideran prácticas óptimas para el manejo de grandes volúmenes de datos y la integración de modelos predictivos que contribuyan a la planificación eficiente de envíos y ventas.  \n","\n","Gracias al trabajo coordinado del equipo PAIT#11, este enfoque estructurado permitirá no solo replicar el proceso con facilidad sino también optimizar la gestión de la información y fortalecer la capacidad analítica del Departamento de Circulación.  \n","\n","Hojas de Datos Utilizadas:\n","\n","\n","1.   **Circulación_2021.csv**\n","2.   **Circulación_2022.csv**\n","2.   **Circulación_2023.csv**\n","2.   **Circulación_2024.csv**\n","2.   **Circulación_2025.csv**\n","\n","\n","\n","*Este material fue desarrollado por el Equipo PAIT#11 en conjunto con Jaime Guell (Investigador de INCAE).*\n"],"metadata":{"id":"ZWETEdRre_o-"}},{"cell_type":"markdown","source":["### **Combinación de Archivos CSV en un Único DataFrame**\n","\n","\n","El código utiliza la biblioteca `pandas` para cargar múltiples archivos CSV (`circulacion_2021.csv` a `circulacion_2025.csv`), combinándolos en un único `DataFrame` mediante la función `pd.concat()`. La opción `ignore_index=True` renumera los índices para evitar duplicados. El `DataFrame` resultante se guarda como `circulacion.csv` sin incluir el índice original. Finalmente, se imprime un mensaje confirmando la operación."],"metadata":{"id":"a_XnW8f5pkTD"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Lista con los nombres de los archivos\n","archivos = [\"circulacion_2021.csv\", \"circulacion_2022.csv\", \"circulacion_2023.csv\", \"circulacion_2024.csv\", \"circulacion_2025.csv\"]\n","\n","# Cargar y combinar los CSV\n","merged_df = pd.concat([pd.read_csv(archivo) for archivo in archivos], ignore_index=True)\n","\n","# Guardar el resultado\n","merged_df.to_csv(\"circulacion.csv\", index=False)\n","\n","print(\"Merge completado y guardado como 'Circulcion_Merge_2021_2025.csv'.\")\n"],"metadata":{"collapsed":true,"id":"3WqeyX93f9Hz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Adición de Columna con el Día de la Semana en Español para Archivos CSV Grandes**\n","\n","El código utiliza `pandas` para procesar grandes volúmenes de datos en bloques (`chunksize=100000`), evitando problemas de memoria al trabajar con el archivo `circulacion.csv`. Convierte la columna `fecha_edicion` al formato `datetime` y crea una nueva columna `dia_semana` en español, usando un diccionario para traducir los días de la semana. La columna se guarda en minúsculas. Los resultados se escriben incrementalmente en `circulacion_dia.csv` utilizando el modo `'a'` (append) y controlando el encabezado para evitar duplicaciones. Finalmente, imprime un mensaje confirmando el éxito de la operación."],"metadata":{"id":"TcH6-3EZSSw0"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Lista con los nombres de los archivos\n","archivos = [\"circulacion.csv\"]\n","\n","# Diccionario para traducir días al español\n","dias_semana_es = {\n","    'Monday': 'lunes',\n","    'Tuesday': 'martes',\n","    'Wednesday': 'miércoles',\n","    'Thursday': 'jueves',\n","    'Friday': 'viernes',\n","    'Saturday': 'sábado',\n","    'Sunday': 'domingo'\n","}\n","\n","# Nombre del archivo de salida\n","output_file = \"circulacion_dia.csv\"\n","\n","# Borrar el archivo si existe para empezar limpio\n","open(output_file, 'w').close()\n","\n","# Procesar cada archivo en bloques\n","for archivo in archivos:\n","    # Carga por bloques para manejar grandes volúmenes de datos\n","    for chunk in pd.read_csv(archivo, chunksize=100000):\n","        # Convertir 'fecha_edicion' a formato datetime\n","        chunk['fecha_edicion'] = pd.to_datetime(chunk['fecha_edicion'], format='%m/%d/%Y', errors='coerce')\n","\n","        # Crear columna 'dia_semana' en español y en minúsculas\n","        chunk['dia_semana'] = chunk['fecha_edicion'].dt.day_name().map(dias_semana_es).str.lower()\n","\n","        # Guardar en CSV de forma incremental\n","        chunk.to_csv(output_file, mode='a', index=False, header=not bool(open(output_file).read(1)))\n","\n","print(\"Merge completado y guardado como 'circulacion.csv' con la columna 'dia_semana' en español y minúsculas.\")\n"],"metadata":{"collapsed":true,"id":"NzWWRjq_ponE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Extracción de Año, Mes y Día desde la Fecha en un Archivo CSV**\n","\n","El código carga el archivo `circulacion_dia.csv` en un `DataFrame` de `pandas` y convierte la columna `fecha_edicion` a formato `datetime` para facilitar la manipulación de fechas. A partir de esta columna, crea tres nuevas columnas: `año`, `mes` y `dia`, extrayendo respectivamente el año, el mes y el día. El `DataFrame` modificado se guarda como `circulacion_fecha.csv` sin incluir el índice original. Finalmente, muestra las primeras filas del `DataFrame` para verificar el resultado."],"metadata":{"id":"SCwtM7EPSe4j"}},{"cell_type":"code","source":["# Cargar el archivo CSV con las columnas ya filtradas\n","df = pd.read_csv(\"circulacion_dia.csv\")\n","\n","# Convertir la columna fecha_edicion a formato datetime\n","df[\"fecha_edicion\"] = pd.to_datetime(df[\"fecha_edicion\"])\n","\n","# Crear nuevas columnas para año, mes y día\n","df[\"año\"] = df[\"fecha_edicion\"].dt.year\n","df[\"mes\"] = df[\"fecha_edicion\"].dt.month\n","df[\"dia\"] = df[\"fecha_edicion\"].dt.day\n","\n","# Guardar el resultado en un nuevo archivo CSV\n","df.to_csv(\"circulacion_fecha.csv\", index=False)\n","\n","# Mostrar las primeras filas del DataFrame\n","print(df.head())"],"metadata":{"collapsed":true,"id":"tsVsM69OyZyr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Reordenamiento Personalizado de Columnas en un Archivo CSV**\n","\n","El código carga el archivo `circulacion_fecha.csv` en un `DataFrame` de `pandas` y reorganiza las columnas según un orden personalizado dividido en categorías lógicas:\n","1. **Variables Temporales:** `fecha_edicion`, `año`, `mes`, `dia`, `dia_semana`.\n","2. **Variables de Pedido y Transacción:** `idorder`, `idpublication`, `publicacion`.\n","3. **Variables Geográficas:** `idregional`, `regional`, `iddeptogeo`, `departamento`, etc.\n","4. **Variables de Canal y Segmento:** `idchannel`, `nomCanal`, `idsegmento`.\n","5. **Variables de Ruta y Logística:** `idroute`, `ruta`, etc.\n","6. **Variables de Clientes y Personal:** `ordinario`, `adicional`, `posterior`, etc.\n","\n","El `DataFrame` reorganizado se guarda como `circulacion_ordenada.csv` sin el índice original. Finalmente, se imprime un mensaje confirmando el éxito de la operación."],"metadata":{"id":"BEzha3zPSrqa"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Cargar el archivo CSV original\n","df = pd.read_csv(\"circulacion_fecha.csv\")\n","\n","# Definir el nuevo orden de las columnas\n","columnas_ordenadas = [\n","    # 1. Variables Temporales\n","    'fecha_edicion', 'año', 'mes', 'dia', 'dia_semana',\n","\n","    # 2. Variables de Pedido y Transacción\n","    'idorder', 'idpublication', 'publicacion',\n","\n","    # 3. Variables Geográficas\n","    'idregional', 'regional', 'iddeptogeo', 'departamento', 'idmupiogeo',\n","    'municipio', 'idsector', 'sector', 'direccion', 'zona',\n","\n","    # 4. Variables de Canal y Segmento\n","    'idchannel', 'nomCanal', 'idsegmento',\n","\n","    # 5. Variables de Ruta y Logística\n","    'idroute', 'ruta', 'idcustomer', 'codigo_sectorista', 'sectorista',\n","    'idsupervisor', 'supervisor',\n","\n","    # 6. Variables de Clientes y Personal\n","    'ordinario', 'adicional', 'posterior',\n","    'promo', 'flete', 'envio_total', 'cobrable', 'devuelto', 'vendido'\n","]\n","\n","# Reordenar las columnas\n","df = df[columnas_ordenadas]\n","\n","# Guardar el DataFrame reordenado en un nuevo archivo\n","df.to_csv(\"circulacion_ordenada.csv\", index=False)\n","\n","print(\"Las columnas han sido reordenadas y guardadas como 'circulacion_ordenada.csv'.\")\n"],"metadata":{"collapsed":true,"id":"6EqfixXwrsj-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Extracción de Componentes de Fecha y Actualización de CSV**\n","\n","El código carga el archivo `circulacion_ordenada.csv` en un `DataFrame` utilizando `pandas` y convierte la columna `fecha_edicion` al formato `datetime` para facilitar la manipulación de fechas. A partir de esta columna, genera tres nuevas columnas:\n","- **`año`**: Extrae el año de `fecha_edicion`.\n","- **`mes`**: Extrae el mes.\n","- **`dia`**: Extrae el día.\n","\n","Luego, guarda el `DataFrame` actualizado en un archivo llamado `salida.csv` sin incluir el índice original. Finalmente, muestra las primeras filas del `DataFrame` para verificar la correcta ejecución del proceso."],"metadata":{"id":"2elm4wCYS40B"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Cargar el archivo CSV con las columnas ya filtradas\n","df = pd.read_csv(\"circulacion_ordenada.csv\")\n","\n","# Convertir la columna fecha_edicion a formato datetime\n","df[\"fecha_edicion\"] = pd.to_datetime(df[\"fecha_edicion\"])\n","\n","# Crear nuevas columnas para año, mes y día\n","df[\"año\"] = df[\"fecha_edicion\"].dt.year\n","df[\"mes\"] = df[\"fecha_edicion\"].dt.month\n","df[\"dia\"] = df[\"fecha_edicion\"].dt.day\n","\n","# Guardar el resultado en un nuevo archivo CSV\n","df.to_csv(\"salida.csv\", index=False)\n","\n","# Mostrar las primeras filas del DataFrame\n","print(df.head())"],"metadata":{"collapsed":true,"id":"PqEBzqXoyOZd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Eliminación de Columnas Específicas en un CSV**\n","\n","\n","El código carga el archivo `salida.csv` en un `DataFrame` utilizando `pandas` y elimina las columnas `fecha_edicion`, `idpublication` y `publicacion` mediante la función `drop(columns=[...])`. Posteriormente, guarda el `DataFrame` modificado en un nuevo archivo llamado `salida1.csv` sin incluir el índice original. Finalmente, imprime un mensaje confirmando que las columnas han sido eliminadas y el archivo guardado correctamente."],"metadata":{"id":"I5TSzQHmTB9A"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Cargar el archivo CSV\n","archivo = 'salida.csv'\n","df = pd.read_csv(archivo)\n","\n","# Hacer el drop de las columnas especificadas\n","df = df.drop(columns=['fecha_edicion', 'idpublication', 'publicacion'])\n","\n","# Guardar el DataFrame modificado en un nuevo archivo CSV\n","df.to_csv('salida1.csv', index=False)\n","\n","print(\"Las columnas 'fecha_edicion', 'idpublicacion' y 'publicacion' han sido eliminadas y guardadas en 'datos_sin_columnas.csv'.\")\n"],"metadata":{"collapsed":true,"id":"g9uE3srzQNTM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Merge : Promociones, Feriados y Hoja madre (salida1.csv)**\n","\n","\n"],"metadata":{"id":"VEBt2hwDmkPU"}},{"cell_type":"markdown","source":["### Carga y Preparación de Datos desde Múltiples CSV en Pandas\n","\n","\n","El código utiliza la biblioteca `pandas` para cargar tres archivos CSV: `salida1.csv`, `promociones.csv` y `dias_festivos_guatemala_2021_2025.csv`, especificando la codificación `latin-1` para evitar problemas con caracteres especiales. A continuación, convierte las columnas de fechas (`fecha_edicion`, `fecha_inicio`, `fecha_fin`, y `Fecha`) al tipo `datetime` para garantizar un formato consistente y facilitar operaciones basadas en fechas.\n","\n","Luego, obtiene los valores únicos de la columna `Tipo` de `hoja_2` y los almacena en la lista `unique_tipo`, que probablemente se utilizará para generar columnas adicionales en futuros procesos. También extrae las fechas únicas de `hoja_1` y las guarda en `unique_dates`, preparando los datos para análisis adicionales o uniones basadas en fechas."],"metadata":{"id":"YysXhAOntn_L"}},{"cell_type":"code","source":["# Load required libraries\n","import pandas as pd\n","\n","# Load csv files\n","hoja_1 = pd.read_csv('salida1.csv', encoding='latin-1')\n","hoja_2 = pd.read_csv('promociones.csv', encoding='latin-1')\n","dias_festivos = pd.read_csv('dias_festivos_guatemala_2021_2025.csv', encoding='latin-1')\n","\n","# Convert date columns to uniform data type\n","hoja_1['fecha_edicion'] = pd.to_datetime(hoja_1['fecha_edicion'], format='%Y-%m-%d')\n","\n","hoja_2['fecha_inicio'] = pd.to_datetime(hoja_2['fecha_inicio'], format='%Y-%m-%d')\n","hoja_2['fecha_fin'] = pd.to_datetime(hoja_2['fecha_fin'], format='%Y-%m-%d')\n","\n","dias_festivos['Fecha'] = pd.to_datetime(dias_festivos['Fecha'], format='%Y-%m-%d')\n","\n","# Get unique values for Tipo from hoja_2. These values will be used for\n","# generating additional columns in the output file\n","unique_tipo = list(set(hoja_2['Tipo']))\n","\n","# Get unique dates from hoja 1\n","unique_dates = hoja_1['fecha_edicion'].unique()\n"],"metadata":{"id":"T-CUHYuQmvmV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Construyendo un DataFrame auxiliar para ayudar.\n","\n","Primero, una columna de fechas únicas ubicada en `hoja_1`.\n","Segundo, una columna para 'Feriados', donde se establecerá un 1 si la fecha única coincide con una fecha en el DataFrame `dias_festivos`.\n","Tercero, se crearán columnas para cada elemento en `unique_tipo`, con valores de 0."],"metadata":{"id":"JaU2pb7PnCJi"}},{"cell_type":"markdown","source":["### Creación de un DataFrame Auxiliar con Indicadores de Feriados y Tipos\n","\n","\n","El código crea un `DataFrame` auxiliar (`helper_df`) a partir de las fechas únicas (`unique_dates`) obtenidas previamente, asignándolas a una columna llamada `date`. Luego, agrega una columna binaria llamada `Feriados` que indica si cada fecha corresponde a un día festivo, basándose en el `DataFrame` `dias_festivos`. Esta columna toma el valor `1` si la fecha está presente en `dias_festivos['Fecha']` y `0` en caso contrario.\n","\n","Posteriormente, el código añade una columna para cada valor único de `Tipo` presente en `unique_tipo`, inicializando todas las celdas de estas columnas en `0`. Esto prepara el `DataFrame` para almacenar información específica relacionada con cada `Tipo` en futuros procesos de análisis o agregación."],"metadata":{"id":"ioxYYKXht2eL"}},{"cell_type":"code","source":["# Building helper data frame\n","helper_df = pd.DataFrame(\n","    {'date': unique_dates}\n",")\n","\n","# Add 'Feriados' column to helper dataframe\n","helper_df['Feriados'] = helper_df['date'].isin(dias_festivos['Fecha']).astype(int)\n","\n","# Add a column for each 'Tipo'\n","for tipo in unique_tipo:\n","    helper_df[tipo] = 0\n","\n",""],"metadata":{"id":"i94VUyj5nEwp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Construyendo otro DataFrame intermedio basado en `hoja_2` para facilitar el trabajo.\n","\n","- Agrupar por 'Tipo'.\n","- Obtener la fecha más baja de `fecha_inicio` y la fecha más alta de `fecha_fin` para períodos de tiempo contiguos por 'Tipo'."],"metadata":{"id":"A2IVt6O6vqRh"}},{"cell_type":"markdown","source":["### Colapso de Intervalos de Fechas por Tipo\n","\n","\n","El código combina intervalos de fechas superpuestos o adyacentes en el `DataFrame` `hoja_2`, agrupados por `Tipo`.\n","\n","- `collapse_intervals(intervals)`:\n","  Ordena los intervalos por fecha de inicio y los fusiona si se superponen o son adyacentes.\n","\n","- `collapse_group(type_group)`:\n","  Aplica `collapse_intervals` a cada grupo de `Tipo` y devuelve un `DataFrame` con los intervalos colapsados.\n","\n","- Resultado:\n","  El `DataFrame` `hoja_2_collapsed` contiene intervalos optimizados y únicos por tipo."],"metadata":{"id":"A5ZWEfxut_36"}},{"cell_type":"code","source":["# Helper function to collapse intervals. Given a list of intervals (tuple of start, end),\n","# merge overlaping and adjacent intervals\n","def collapse_intervals(intervals):\n","    # Sort intervals by start date\n","    intervals.sort(key=lambda x: x[0])\n","    collapsed = []\n","\n","    for current_interval in intervals:\n","        if not collapsed:\n","            collapsed.append(list(current_interval))\n","        else:\n","            # Check if current interval starts within or right after the last merged interval\n","            # If so, merge them.\n","            if current_interval[0] <= collapsed[-1][1] + pd.Timedelta(days=1):\n","                # Merge the intervals by extending the end date if necessaru\n","                collapsed[-1][1] = max(collapsed[-1][1], current_interval[1])\n","            else:\n","                collapsed.append(list(current_interval))\n","\n","    return collapsed\n","\n","\n","# Function to merge intervals for each group of p_type\n","def collapse_group(type_group):\n","    # Get group name\n","    group_name = type_group.name\n","    # Create a list of (start_Data, end_date) tuples for the group\n","    intervals = list(zip(type_group['fecha_inicio'], type_group['fecha_fin']))\n","    collapsed = collapse_intervals(intervals)\n","\n","    collapsed_df = pd.DataFrame({\n","        'Tipo': [group_name] * len(collapsed),\n","        'fecha_inicio': [interval[0] for interval in collapsed],\n","        'fecha_fin': [interval[1] for interval in collapsed]\n","    })\n","\n","    return collapsed_df\n","\n","hoja_2_collapsed = hoja_2.groupby('Tipo', group_keys=False)[['fecha_inicio', 'fecha_fin']].apply(collapse_group)"],"metadata":{"id":"4DYRnsDdnbr7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hagamos algo mágico:\n","\n","Ahora, para todas las fechas únicas en `helper_df`, se verificará si caen dentro del rango por 'Tipo' definido en `hoja_2_collapsed`, y se establecerá un 1 en la columna correspondiente de `helper_df`.  "],"metadata":{"id":"cmJJFG56nu4Q"}},{"cell_type":"markdown","source":["### Marcado de Fechas dentro de Intervalos para Cada Tipo\n","\n","El código recorre cada fila del `DataFrame` `hoja_2_collapsed`, que contiene columnas: `Tipo`, `fecha_inicio` y `fecha_fin`. Para cada intervalo de fechas:\n","\n","1. Obtiene los valores:\n","   Extrae el tipo (`tipo_h`), la fecha de inicio (`start_date`) y la fecha de fin (`end_date`).\n","\n","2. Crea una máscara booleana (`in_range`):\n","   Genera una condición booleana para identificar las filas en `helper_df` donde la columna `date` cae dentro del rango definido por `start_date` y `end_date`.\n","\n","3. Asigna valores a `helper_df`:  \n","   Utiliza `.loc` para actualizar la columna correspondiente a `tipo_h` en `helper_df`, asignando `1` a las fechas que cumplen la condición y manteniendo `0` en las demás.\n","\n","\n","### Resultado:\n","El `DataFrame` `helper_df` se completa con indicadores binarios (`1` o `0`) para cada tipo, señalando si una fecha específica se encuentra dentro de los intervalos establecidos en `hoja_2_collapsed`. Esto permite identificar rápidamente la aplicabilidad de cada tipo en función de las fechas."],"metadata":{"id":"XMj8Zbr2ubrt"}},{"cell_type":"code","source":["for idx, row in hoja_2_collapsed.iterrows():\n","    tipo_h = row['Tipo']\n","    start_date = row['fecha_inicio']\n","    end_date = row['fecha_fin']\n","\n","    # Boolean mask to check if date is within range\n","    in_range = (helper_df['date'] >= start_date) & (helper_df['date'] <= end_date)\n","\n","    # Set corresponding column to 1 where the condition is met\n","    helper_df.loc[in_range, tipo_h] = 1"],"metadata":{"id":"xWk4jnQFnyIX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The helper_df is complete. This df has for every unique date that appeared in hoja_1:\n","* A column with the dates\n","* A column for 'Feriados': set to 1 if it matches a date in the dias_festivos\n","* Columns for each unique Tipo from hoja 2: with it's corresponding Tipo set to 1 if there is a match in the date ranges from hoja_2\n","\n","\n","Now, the helper_df will be joined to hoja_1 to produce the expected outcome"],"metadata":{"id":"tF-kv3bin0w1"}},{"cell_type":"markdown","source":["### Unificación de Datos mediante Merge\n","\n","Renombra `date` a `fecha_edicion` en `helper_df` y une `hoja_1` con `helper_df` usando `merge` en modo `left`. Guarda el resultado como `hoja_1_enhanced.csv`, agregando las columnas adicionales sin perder filas originales."],"metadata":{"id":"feLrwr9QutXG"}},{"cell_type":"code","source":["# Pre-work: rename date column from helper_df to fecha edicion, in order to hava a cleaner result after merging\n","helper_df = helper_df.rename(columns={'date': 'fecha_edicion'})\n","\n","# Merge the data\n","hoja_1_enhanced = hoja_1.merge(helper_df, how='left', on='fecha_edicion')\n","\n","# Save to output file\n","hoja_1_enhanced.to_csv('hoja_1_enhanced.csv', index=False)"],"metadata":{"id":"JTRcZK7noAsH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Merge: Hoja Madre (hoja_1_enhanced.csv)**"],"metadata":{"id":"UmmMCEqczw3u"}},{"cell_type":"markdown","source":["Integración y Limpieza de Datos con Pandas: Uniendo Archivos CSV y Excel Basados en Fechas y Municipios\n","\n","Este código carga dos archivos (un CSV y un Excel) en DataFrames usando `pandas`, normaliza los nombres de columnas y municipios (minúsculas y sin espacios), y asegura que las fechas estén en el mismo formato. Luego, transforma los datos de lluvia usando `melt()` para tener una estructura uniforme con las columnas `fecha_edicion`, `municipio` y `lluvia`.\n","\n","Después, filtra las filas para mantener solo las coincidencias exactas de municipio y fecha entre los DataFrames. Realiza un `merge` tipo `left` para combinar los datos, rellenando los valores faltantes en la columna `lluvia` con 0. Finalmente, guarda el DataFrame resultante en un nuevo CSV llamado `datos_limpios_con_temp.csv`."],"metadata":{"id":"jQpb7lpa1bdm"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Definir rutas de los archivos\n","ruta_hoja_madre = \"hoja_1_enhanced.csv.csv\"\n","ruta_hoja_temp = \"Temp match.xlsx\"\n","ruta_salida_csv = \"datos_limpios_con_temp.csv\"\n","\n","# Crear el directorio de salida si no existe\n","ruta_directorio = os.path.dirname(ruta_salida_csv)\n","if not os.path.exists(ruta_directorio):\n","    os.makedirs(ruta_directorio)\n","\n","# Cargar los datos\n","print(\"Cargando datos...\")\n","hoja_madre = pd.read_csv(ruta_hoja_madre)\n","hoja_temp = pd.read_excel(ruta_hoja_temp)\n","\n","# Normalizar nombres de columnas eliminando espacios y convirtiendo a minúsculas\n","print(\"Normalizando nombres de columnas y municipios...\")\n","hoja_madre.columns = hoja_madre.columns.str.strip().str.lower()\n","hoja_temp.columns = hoja_temp.columns.str.strip().str.lower()\n","\n","# Asegurar que la fecha tiene el mismo formato\n","print(\"Formateando fechas...\")\n","hoja_madre[\"fecha_edicion\"] = pd.to_datetime(hoja_madre[\"fecha_edicion\"], errors='coerce')\n","hoja_temp[\"fecha_edicion\"] = pd.to_datetime(hoja_temp[\"fecha_edicion\"], errors='coerce')\n","\n","# Transformar la hoja de lluvia para que cada municipio sea una fila con su valor de lluvia\n","print(\"Transformando hoja de lluvia con melt()...\")\n","hoja_lluvia = hoja_temp.melt(id_vars=[\"fecha_edicion\"], var_name=\"municipio\", value_name=\"lluvia\")\n","\n","# Normalizar nombres de municipios eliminando espacios extra\n","if \"municipio\" in hoja_madre.columns and \"municipio\" in hoja_lluvia.columns:\n","    hoja_madre[\"municipio\"] = hoja_madre[\"municipio\"].str.strip().str.lower()\n","    hoja_lluvia[\"municipio\"] = hoja_lluvia[\"municipio\"].str.strip().str.lower()\n","else:\n","    print(\"Error: La columna 'municipio' no se encuentra en uno de los DataFrames.\")\n","    print(\"Columnas en hoja_madre:\", hoja_madre.columns.tolist())\n","    print(\"Columnas en hoja_lluvia:\", hoja_lluvia.columns.tolist())\n","    exit()\n","\n","# Verificación previa al merge\n","print(\"Ejemplo de municipios en hoja madre:\", hoja_madre[\"municipio\"].unique()[:10])\n","print(\"Ejemplo de municipios en hoja lluvia:\", hoja_lluvia[\"municipio\"].unique()[:10])\n","\n","# Filtrar solo los municipios y fechas que realmente tienen coincidencia\n","print(\"Filtrando solo coincidencias exactas antes del merge...\")\n","hoja_lluvia_filtrada = hoja_lluvia[\n","    hoja_lluvia[\"municipio\"].isin(hoja_madre[\"municipio\"].unique()) &\n","    hoja_lluvia[\"fecha_edicion\"].isin(hoja_madre[\"fecha_edicion\"].unique())\n","]\n","\n","# Realizar el merge asegurando coincidencias exactas de municipio y fecha\n","print(\"Realizando merge de datos con coincidencia exacta de fecha y municipio...\")\n","hoja_madre = hoja_madre.merge(hoja_lluvia_filtrada, on=[\"fecha_edicion\", \"municipio\"], how=\"left\")\n","\n","# Asignar 0 a los valores sin coincidencia\n","hoja_madre[\"lluvia\"] = hoja_madre[\"lluvia\"].fillna(0)\n","\n","# Verificar valores después del merge\n","print(\"Ejemplo de datos después del merge:\")\n","print(hoja_madre[[\"fecha_edicion\", \"municipio\", \"lluvia\"]].head(10))\n","\n","# Guardar en un único archivo CSV sin restricciones de tamaño\n","print(\"Guardando CSV corregido...\")\n","hoja_madre.to_csv(ruta_salida_csv, index=False)\n","\n","print(f\"CSV guardado exitosamente en {ruta_salida_csv}\")"],"metadata":{"id":"CSTjOh_7ERCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exportación y Muestreo de Datos con Pandas en Python\n","\n","Este código carga un archivo CSV llamado `datos_limpios_con_temp.csv` en un DataFrame y normaliza los nombres de columnas a minúsculas y sin espacios, asegurando también que la columna `fecha_edicion` tenga el formato datetime correcto. Luego, toma una muestra aleatoria de 2000 filas y la guarda en dos formatos diferentes: CSV (`datos_muestra_2000.csv`) y Excel (`datos_muestra_2000.xlsx`). Además, exporta el DataFrame completo a un archivo CSV (`datos_limpios_con_temp_completo.csv`).\n","\n","Para manejar grandes volúmenes de datos en Excel, el código utiliza `pd.ExcelWriter` y divide el DataFrame en varias hojas si supera el límite de 1,000,000 filas, guardando el resultado en `datos_limpios_con_temp.xlsx`. Finalmente, muestra un ejemplo de las primeras filas de la muestra aleatoria para verificar el contenido exportado."],"metadata":{"id":"d4l4e9zVHVPq"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Cargar los datos completos desde la nueva hoja de datos\n","hoja_madre = pd.read_csv(\"/content/datos_limpios_con_temp.csv\")\n","\n","# Normalizar nombres de columnas eliminando espacios y convirtiendo a minúsculas\n","hoja_madre.columns = hoja_madre.columns.str.strip().str.lower()\n","\n","# Asegurar que la fecha tiene el mismo formato\n","hoja_madre[\"fecha_edicion\"] = pd.to_datetime(hoja_madre[\"fecha_edicion\"], errors='coerce')\n","\n","# Tomar una muestra aleatoria de 2000 observaciones\n","hoja_muestra = hoja_madre.sample(n=2000, random_state=42)\n","\n","# Guardar la muestra en un archivo CSV\n","hoja_muestra.to_csv(\"/content/datos_muestra_2000.csv\", index=False)\n","\n","# Guardar la muestra en un archivo Excel\n","hoja_muestra.to_excel(\"/content/datos_muestra_2000.xlsx\", index=False, engine=\"openpyxl\")\n","\n","# Guardar en un único archivo CSV todo el dataset\n","hoja_madre.to_csv(\"/content/datos_limpios_con_temp_completo.csv\", index=False)\n","\n","# Guardar en un único archivo Excel dividiendo en hojas si es necesario\n","max_filas = 1000000\n","with pd.ExcelWriter(\"/content/datos_limpios_con_temp.xlsx\", engine=\"openpyxl\") as writer:\n","    for i in range(0, len(hoja_madre), max_filas):\n","        hoja_madre.iloc[i:i+max_filas].to_excel(writer, sheet_name=f\"Parte_{i//max_filas+1}\", index=False)\n","\n","# Mostrar las primeras filas de la muestra\n","print(\"Ejemplo de datos de la muestra aleatoria de 2000 observaciones:\")\n","print(hoja_muestra.head())"],"metadata":{"id":"WZ8imcSqFIWm"},"execution_count":null,"outputs":[]}]}